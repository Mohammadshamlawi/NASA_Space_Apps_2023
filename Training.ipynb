{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-cTi2jDThj4T"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import torchvision\n",
        "from torchvision.models.segmentation import deeplabv3_mobilenet_v3_large\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "import tifffile\n",
        "\n",
        "class GeoDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None, train=True, header=None):\n",
        "        self.img_target = pd.read_csv(csv_file, header=header)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.train = train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_target)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.root_dir, self.img_target.iloc[index, 0])\n",
        "        image = tifffile.imread(img_path)\n",
        "\n",
        "        label_path = os.path.join(self.root_dir, self.img_target.iloc[index, 1])\n",
        "        y_label = tifffile.imread(label_path)\n",
        "\n",
        "        image = image[:, :, :3]\n",
        "\n",
        "        if self.transform:\n",
        "            auggmented = self.transform(image = np.array(image), mask= np.array(y_label))\n",
        "            image, mask = auggmented['image'], auggmented['mask']\n",
        "\n",
        "        return image, mask+1\n"
      ],
      "metadata": {
        "id": "C31IdtZyi9ar"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_CLASSES = 3\n",
        "TRAIN_VAL_SPLIT = 0.8\n",
        "HEIGHT = 512\n",
        "WIDTH  = 512\n",
        "\n",
        "batch_size = 8\n",
        "num_epochs = 10\n",
        "\n",
        "train_transforms = A.Compose([\n",
        "    A.Resize(height=256, width=256, p=1.0),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.Rotate(limit=30, p=0.5),\n",
        "    A.Normalize(mean=(0.1759, 0.2107, 0.2502), std=(0.3352, 0.3322, 0.3726)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "\n",
        "val_transforms = A.Compose([\n",
        "    A.Resize(height=256, width=256, p=1.0),\n",
        "    A.Normalize(mean=(0.0511, 0.0766, 0.0931), std=(0.0292, 0.0357, 0.0528)),\n",
        "    ToTensorV2()\n",
        "])"
      ],
      "metadata": {
        "id": "NbNXY7Vviycs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "T58vijBhi5PD",
        "outputId": "b5f2bf04-cb3c-4248-bb79-73462c701e7a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "traing_dataset = GeoDataset(\"/content/drive/MyDrive/GeoAI/training_index.csv\", root_dir=\"/content/drive/MyDrive/GeoAI/training\",\n",
        "                             transform=train_transforms, train=True)\n",
        "\n",
        "train_loader = DataLoader(traing_dataset, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "MPJgW2Pbi6p0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = GeoDataset(\"/content/drive/MyDrive/GeoAI/validation_index.csv\", root_dir=\"/content/drive/MyDrive/GeoAI/validation\",\n",
        "                             transform=val_transforms, train=False)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "jGLlB6rPjA1q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def f1_dice_score(preds, true_mask):\n",
        "    '''\n",
        "    https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2\n",
        "    preds should be (B, 25, H, W)\n",
        "    true_mask should be (B, H, W)\n",
        "    '''\n",
        "\n",
        "    f1_batch = []\n",
        "\n",
        "    for i in range(len(preds)):\n",
        "        f1_image = []\n",
        "        img = preds[i].to(DEVICE)\n",
        "        mask = true_mask[i].to(DEVICE)\n",
        "\n",
        "        # Change shape of img from [25, H, W] to [H, W]\n",
        "        img = torch.argmax(img, dim=0)\n",
        "\n",
        "        for label in range(3):\n",
        "            if torch.sum(mask == label) != 0:\n",
        "                area_of_intersect = torch.sum((img == label) * (mask == label))\n",
        "                area_of_img = torch.sum(img == label)\n",
        "                area_of_label = torch.sum(mask == label)\n",
        "                f1 = 2 * area_of_intersect / (area_of_img + area_of_label)\n",
        "                f1_image.append(f1)\n",
        "\n",
        "        f1_batch.append(np.mean([tensor.cpu() for tensor in f1_image]))\n",
        "    return np.mean(f1_batch)"
      ],
      "metadata": {
        "id": "X91ve30KjHwC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def accuracy(preds, true_mask):\n",
        "    '''\n",
        "    preds should be (B, C, H, W)\n",
        "    true_mask should be (B, H, W)\n",
        "    '''\n",
        "    # Ensure preds and true_mask are on the same device\n",
        "    preds = preds.to(true_mask.device)\n",
        "\n",
        "    # Calculate the number of classes (number of channels in preds)\n",
        "    num_classes = preds.size(1)\n",
        "\n",
        "    accuracy_batch = []\n",
        "\n",
        "    for i in range(len(preds)):\n",
        "        img = torch.argmax(preds[i], dim=0)\n",
        "        mask = true_mask[i]\n",
        "\n",
        "        class_accuracy = []\n",
        "\n",
        "        for c in range(num_classes):\n",
        "            # Calculate accuracy for each class separately\n",
        "            class_pred = (img == c)\n",
        "            class_mask = (mask == c)\n",
        "            class_correct = (class_pred & class_mask).sum().item()\n",
        "            class_total = class_mask.sum().item()\n",
        "\n",
        "            # Avoid division by zero\n",
        "            if class_total > 0:\n",
        "                class_accuracy.append(class_correct / class_total)\n",
        "            else:\n",
        "                class_accuracy.append(0.0)\n",
        "\n",
        "        # Average accuracy over classes\n",
        "        average_accuracy = sum(class_accuracy) / num_classes\n",
        "\n",
        "        accuracy_batch.append(average_accuracy)\n",
        "\n",
        "    return torch.mean(torch.tensor(accuracy_batch))\n"
      ],
      "metadata": {
        "id": "1aN9guKWjHyq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    min_val_f1 = 0.3\n",
        "\n",
        "    for epoch in range(STARTING_EPOCH + 1, STARTING_EPOCH + EPOCHS + 1):\n",
        "\n",
        "        # Train model\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        train_accuracy = []\n",
        "        train_f1 = []\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            # Extract data, labels\n",
        "            img_batch, mask_batch = batch  # img [B,3,H,W], mask[B,H,W]\n",
        "\n",
        "            # Train model\n",
        "            optimizer.zero_grad()\n",
        "            # with torch.cuda.amp.autocast():\n",
        "            output = model(img_batch.to(DEVICE))  # output: [B, 25, H, W]\n",
        "\n",
        "            pred = output\n",
        "            pred = pred.to('cpu')\n",
        "\n",
        "            mask_batch = mask_batch.type(torch.LongTensor).to('cpu')\n",
        "\n",
        "            loss = criterion(pred, mask_batch)\n",
        "            loss.backward()\n",
        "\n",
        "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 6)\n",
        "            optimizer.step()\n",
        "\n",
        "            # Add current loss to temporary list (after 1 epoch take avg of all batch losses)\n",
        "            f1 = f1_dice_score(output, mask_batch)\n",
        "            acc = accuracy(output, mask_batch)\n",
        "            train_losses.append(loss.item())\n",
        "            train_accuracy.append(acc)\n",
        "            train_f1.append(f1)\n",
        "            # print(f'Train Epoch: {epoch}, batch: {i} | Batch metrics | loss: {loss.item():.4f}, f1: {f1:.3f}, accuracy: {acc:.3f}')\n",
        "\n",
        "        # Update global metrics\n",
        "        print(\n",
        "            f'TRAIN       Epoch: {epoch} | Epoch metrics | loss: {np.mean(train_losses):.4f}, f1: {np.mean(train_f1):.3f}, accuracy: {np.mean(train_accuracy):.3f}')\n",
        "        total_train_losses.append(np.mean(train_losses))\n",
        "        total_train_accuracy.append(np.mean(train_accuracy))\n",
        "        total_train_f1.append(np.mean(train_f1))\n",
        "\n",
        "        # Validate model\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        val_accuracy = []\n",
        "        val_f1 = []\n",
        "\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            # Extract data, labels\n",
        "            img_batch, mask_batch = batch\n",
        "            img_batch = img_batch.to(DEVICE)\n",
        "            mask_batch = mask_batch.to(DEVICE)\n",
        "\n",
        "            # Validate model\n",
        "            with torch.cuda.amp.autocast():\n",
        "\n",
        "\n",
        "\n",
        "                output = model(img_batch)\n",
        "                pred = output.to('cuda')\n",
        "                mask_batch = mask_batch.type(torch.LongTensor).to('cuda')\n",
        "                loss = criterion(pred, mask_batch)\n",
        "\n",
        "            # Add current loss to temporary list (after 1 epoch take avg of all batch losses)\n",
        "            f1 = f1_dice_score(output, mask_batch)\n",
        "            acc = accuracy(output, mask_batch)\n",
        "            val_losses.append(loss.item())\n",
        "            val_accuracy.append(acc)\n",
        "            val_f1.append(f1)\n",
        "\n",
        "            # print(f'Val Epoch: {epoch}, batch: {i} | Batch metrics | loss: {loss.item():.4f}, f1: {f1:.3f}, accuracy: {acc:.3f}')\n",
        "\n",
        "        # Update global metrics\n",
        "        print(\n",
        "            f'VALIDATION  Epoch: {epoch} | Epoch metrics | loss: {np.mean(val_losses):.4f}, f1: {np.mean(val_f1):.3f}, accuracy: {np.mean(val_accuracy):.3f}')\n",
        "        print('---------------------------------------------------------------------------------')\n",
        "        total_val_losses.append(np.mean(val_losses))\n",
        "        total_val_accuracy.append(np.mean(val_accuracy))\n",
        "        total_val_f1.append(np.mean(val_f1))\n",
        "\n",
        "        # Save the model\n",
        "        if np.mean(val_f1) > min_val_f1:\n",
        "            torch.save(model.state_dict(),\n",
        "                       f'/mm/{epoch}.pt')\n",
        "            min_val_f1 = np.mean(val_f1)\n",
        "\n",
        "        # Save the results so far\n",
        "        temp_df = pd.DataFrame(list(zip(total_train_losses, total_val_losses, total_train_f1, total_val_f1,\n",
        "                                        total_train_accuracy, total_val_accuracy)),\n",
        "                               columns=['train_loss', 'val_loss', 'train_f1', 'test_f1', 'train_accuracy',\n",
        "                                        'test_accuracy'])\n",
        "        temp_df.to_csv('train_val_measures')\n",
        "\n"
      ],
      "metadata": {
        "id": "Ijxe_W3rjH1D"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = smp.PSPNet(\n",
        "    encoder_name = 'resnet101',\n",
        "    encoder_weights = 'imagenet',\n",
        "    classes = 3,\n",
        "    activation = None, # could be None for logits or 'softmax2d' for multiclass segmentation\n",
        ").to(DEVICE)"
      ],
      "metadata": {
        "id": "kEgCF1FTjXCW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_train_losses   = []\n",
        "total_val_losses     = []\n",
        "total_train_accuracy = []\n",
        "total_val_accuracy   = []\n",
        "total_train_f1       = []\n",
        "total_val_f1         = []\n",
        "\n",
        "\n",
        "# HYPERPARAMETERS for training run\n",
        "STARTING_EPOCH = 10\n",
        "EPOCHS = 20\n",
        "LR = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "\n",
        "train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "WoRYe6YZjXFF",
        "outputId": "ea581bf9-029c-45cd-81c6-dd936dda70c9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN       Epoch: 11 | Epoch metrics | loss: 0.4168, f1: 0.543, accuracy: 0.446\n",
            "VALIDATION  Epoch: 11 | Epoch metrics | loss: 25.6683, f1: 0.420, accuracy: 0.333\n",
            "---------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-36609f017486>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-06e8e16cc372>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtrain_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0;31m# Extract data, labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mimg_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m  \u001b[0;31m# img [B,3,H,W], mask[B,H,W]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-1f7651c52ac7>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtifffile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mlabel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tifffile/tifffile.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(files, selection, aszarr, key, series, level, squeeze, maxworkers, buffersize, mode, name, offset, size, pattern, axesorder, categories, imread, sort, container, chunkshape, dtype, axestiled, ioworkers, chunkmode, fillvalue, zattrs, multiscales, omexml, out, out_inplace, _multifile, _useframes, **kwargs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mzarr_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m                 return tif.asarray(\n\u001b[0m\u001b[1;32m   1107\u001b[0m                     \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m                     \u001b[0mseries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tifffile/tifffile.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(self, key, series, level, squeeze, out, maxworkers, buffersize)\u001b[0m\n\u001b[1;32m   4332\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4333\u001b[0m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4334\u001b[0;31m                 result = self.filehandle.read_array(\n\u001b[0m\u001b[1;32m   4335\u001b[0m                     \u001b[0mtypecode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4336\u001b[0m                     \u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tifffile/tifffile.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(self, dtype, count, offset, out)\u001b[0m\n\u001b[1;32m  14505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  14506\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 14507\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  14508\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  14509\u001b[0m             result[:] = numpy.frombuffer(self._fh.read(nbytes), dtype).reshape(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i4f2WvbwlMGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wmqHlPrslNkN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}